{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install pandas\n",
    "# !pip install gensim\n",
    "# !pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import contractions\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read & View Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SA Training Data.csv\", encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock</th>\n",
       "      <th>News</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc. (AMZN): Ken Fisher’s AI-Driven...</td>\n",
       "      <td>Optimistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon launches drone delivery in Phoenix. Ama...</td>\n",
       "      <td>Optimistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Zacks Investment Ideas feature highlights: Ama...</td>\n",
       "      <td>Optimistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla options draw 'euphoric' trading as Trump...</td>\n",
       "      <td>Optimistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>Why Tesla Stock Keeps Going Up. Tesla (NASDAQ:...</td>\n",
       "      <td>Optimistic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Stock                                               News   Sentiment\n",
       "0  AMZN  Amazon.com Inc. (AMZN): Ken Fisher’s AI-Driven...  Optimistic\n",
       "1  AMZN  Amazon launches drone delivery in Phoenix. Ama...  Optimistic\n",
       "2  AMZN  Zacks Investment Ideas feature highlights: Ama...  Optimistic\n",
       "3  TSLA  Tesla options draw 'euphoric' trading as Trump...  Optimistic\n",
       "4  TSLA  Why Tesla Stock Keeps Going Up. Tesla (NASDAQ:...  Optimistic"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading news articles.\n"
     ]
    }
   ],
   "source": [
    "# Initialize stop word list and lemmatizer\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Initialize empty list to store the sequence of sentiment labels\n",
    "labels = []\n",
    "\n",
    "# Initialize empty list to store the articles where each article is a list of words\n",
    "corpus = []\n",
    "\n",
    "# For every row in the data frame\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    # Extract the label and the text\n",
    "    label = row['Sentiment']\n",
    "    text = row['News']\n",
    "\n",
    "    # Store the label into the list of labels\n",
    "    labels.append(label)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    article = nltk.word_tokenize(text)\n",
    "\n",
    "    # Lowercase conversion\n",
    "    article = [w.lower() for w in article]\n",
    "\n",
    "    # Stop word removal\n",
    "    article = [w for w in article if w not in stop_list]\n",
    "\n",
    "    # Remove punctuation\n",
    "    article = [w for w in article if w.isalnum()]\n",
    "\n",
    "    # Expand contractions\n",
    "    article = [contractions.fix(w) for w in article]\n",
    "\n",
    "    # Lemmatization\n",
    "    article = [lemmatizer.lemmatize(w) for w in article]\n",
    "\n",
    "    # Create bigrams\n",
    "    bigrams = [' '.join(w) for w in list(ngrams(article, 2))]\n",
    "    article.extend(bigrams)\n",
    "\n",
    "    # Store the preprocessed news article into the corpus\n",
    "    corpus.append(article)\n",
    "\n",
    "print('Finished reading news articles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dictionary & Vectorize News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparing the training data.\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from the corpus\n",
    "dictionary = gensim.corpora.Dictionary(corpus)\n",
    "\n",
    "# Store the labelled training data in the following list\n",
    "labelled_training_data = []\n",
    "\n",
    "# Go through the two lists in parallel to create a labelled dataset\n",
    "for (l, a) in zip(labels, corpus):\n",
    "\n",
    "    # Convert the original article into a vector\n",
    "    vector = dictionary.doc2bow(a)\n",
    "\n",
    "    # Create a dictionary object to store the document vector for later use in NLTK's classifier\n",
    "    article_as_dict = {id: 1 for (id, tf) in vector}\n",
    "\n",
    "    # Add the labelled articles to the labelled dataset\n",
    "    labelled_training_data.append((article_as_dict, l))\n",
    "\n",
    "print('Finished preparing the training data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Maximum Entrophy Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -1.09861        0.330\n",
      "             2          -1.09026        1.000\n",
      "             3          -1.08202        1.000\n",
      "             4          -1.07388        1.000\n",
      "             5          -1.06585        1.000\n",
      "             6          -1.05792        1.000\n",
      "             7          -1.05009        1.000\n",
      "             8          -1.04236        1.000\n",
      "             9          -1.03474        1.000\n",
      "            10          -1.02721        1.000\n",
      "            11          -1.01978        1.000\n",
      "            12          -1.01244        1.000\n",
      "            13          -1.00520        1.000\n",
      "            14          -0.99806        1.000\n",
      "            15          -0.99100        1.000\n",
      "            16          -0.98404        1.000\n",
      "            17          -0.97717        1.000\n",
      "            18          -0.97038        1.000\n",
      "            19          -0.96369        1.000\n",
      "            20          -0.95708        1.000\n",
      "            21          -0.95055        1.000\n",
      "            22          -0.94411        1.000\n",
      "            23          -0.93775        1.000\n",
      "            24          -0.93147        1.000\n",
      "            25          -0.92527        1.000\n",
      "            26          -0.91914        1.000\n",
      "            27          -0.91310        1.000\n",
      "            28          -0.90713        1.000\n",
      "            29          -0.90123        1.000\n",
      "            30          -0.89541        1.000\n",
      "            31          -0.88965        1.000\n",
      "            32          -0.88397        1.000\n",
      "            33          -0.87836        1.000\n",
      "            34          -0.87282        1.000\n",
      "            35          -0.86734        1.000\n",
      "            36          -0.86193        1.000\n",
      "            37          -0.85659        1.000\n",
      "            38          -0.85131        1.000\n",
      "            39          -0.84609        1.000\n",
      "            40          -0.84094        1.000\n",
      "            41          -0.83584        1.000\n",
      "            42          -0.83081        1.000\n",
      "            43          -0.82583        1.000\n",
      "            44          -0.82091        1.000\n",
      "            45          -0.81605        1.000\n",
      "            46          -0.81124        1.000\n",
      "            47          -0.80649        1.000\n",
      "            48          -0.80180        1.000\n",
      "            49          -0.79715        1.000\n",
      "            50          -0.79256        1.000\n",
      "            51          -0.78802        1.000\n",
      "            52          -0.78353        1.000\n",
      "            53          -0.77909        1.000\n",
      "            54          -0.77471        1.000\n",
      "            55          -0.77036        1.000\n",
      "            56          -0.76607        1.000\n",
      "            57          -0.76182        1.000\n",
      "            58          -0.75762        1.000\n",
      "            59          -0.75347        1.000\n",
      "            60          -0.74936        1.000\n",
      "            61          -0.74529        1.000\n",
      "            62          -0.74127        1.000\n",
      "            63          -0.73728        1.000\n",
      "            64          -0.73335        1.000\n",
      "            65          -0.72945        1.000\n",
      "            66          -0.72559        1.000\n",
      "            67          -0.72177        1.000\n",
      "            68          -0.71800        1.000\n",
      "            69          -0.71426        1.000\n",
      "            70          -0.71056        1.000\n",
      "            71          -0.70689        1.000\n",
      "            72          -0.70327        1.000\n",
      "            73          -0.69968        1.000\n",
      "            74          -0.69612        1.000\n",
      "            75          -0.69261        1.000\n",
      "            76          -0.68912        1.000\n",
      "            77          -0.68567        1.000\n",
      "            78          -0.68226        1.000\n",
      "            79          -0.67888        1.000\n",
      "            80          -0.67553        1.000\n",
      "            81          -0.67221        1.000\n",
      "            82          -0.66893        1.000\n",
      "            83          -0.66568        1.000\n",
      "            84          -0.66246        1.000\n",
      "            85          -0.65927        1.000\n",
      "            86          -0.65611        1.000\n",
      "            87          -0.65298        1.000\n",
      "            88          -0.64988        1.000\n",
      "            89          -0.64681        1.000\n",
      "            90          -0.64376        1.000\n",
      "            91          -0.64075        1.000\n",
      "            92          -0.63776        1.000\n",
      "            93          -0.63480        1.000\n",
      "            94          -0.63187        1.000\n",
      "            95          -0.62896        1.000\n",
      "            96          -0.62608        1.000\n",
      "            97          -0.62323        1.000\n",
      "            98          -0.62040        1.000\n",
      "            99          -0.61760        1.000\n",
      "         Final          -0.61482        1.000\n"
     ]
    }
   ],
   "source": [
    "# Set number of iterations and define the classifier\n",
    "numIterations = 100\n",
    "algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]\n",
    "\n",
    "# Train the classifier\n",
    "entclassifier = nltk.MaxentClassifier.train(labelled_training_data, algorithm, max_iter=numIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Gensim Training Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Gensim Dictionary using the pickle module\n",
    "with open('dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Gensim Training Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved Gensim Dictionary\n",
    "with open('dictionary.pkl', 'rb') as f:\n",
    "    dictionary_load = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Maximum Entrophy Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model using the pickle module\n",
    "with open('maxent_sentiment_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(entclassifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved classifier model\n",
    "with open('maxent_sentiment_classifier.pkl', 'rb') as f:\n",
    "    classifier_load = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Test News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to predict the sentiment of a new unseen article\n",
    "def predict_sentiment(text):\n",
    "\n",
    "    # Initialize stop word list and lemmatizer\n",
    "    stop_list = nltk.corpus.stopwords.words('english')\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    article = nltk.word_tokenize(text)\n",
    "\n",
    "    # Lowercase conversion\n",
    "    article = [w.lower() for w in article]\n",
    "\n",
    "    # Stop word removal\n",
    "    article = [w for w in article if w not in stop_list]\n",
    "\n",
    "    # Remove punctuation\n",
    "    article = [w for w in article if w.isalnum()]\n",
    "\n",
    "    # Expand contractions\n",
    "    article = [contractions.fix(w) for w in article]\n",
    "\n",
    "    # Lemmatization\n",
    "    article = [lemmatizer.lemmatize(w) for w in article]\n",
    "\n",
    "    # Create bigrams\n",
    "    bigrams = [' '.join(w) for w in list(ngrams(article, 2))]\n",
    "    article.extend(bigrams)\n",
    "\n",
    "    # Convert the original article into a vector\n",
    "    vector = dictionary_load.doc2bow(article)\n",
    "\n",
    "    # Create a dictionary object to store the document vector for later use in NLTK's classifier\n",
    "    article_as_dict = {id: 1 for (id, tf) in vector}\n",
    "\n",
    "    # Predict the sentiment of the news article\n",
    "    return classifier_load.classify(article_as_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input test sample to predict sentiment of news article\n",
    "text = input()\n",
    "print(predict_sentiment(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
